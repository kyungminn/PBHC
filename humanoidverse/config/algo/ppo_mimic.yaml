# @package _global_

algo:
  _target_: humanoidverse.agents.ppo.ppo_mimic.PPO
  _recursive_: False
  config:
    num_learning_epochs: 5
    num_mini_batches: 4
    clip_param: 0.2
    gamma: 0.99
    lam: 0.95
    value_loss_coef: 1.0
    entropy_coef: 0.005 #0.005
    l2c2:
      enable: False
      lambda_value: 1.0
      lambda_policy: 0.1
    

    # teacher-student related config
    dagger_only: False
    dagger_update_freq: 20
    teacher_model_path: null
    
    learning_rate: 1.e-4 # 5e-4 # 1.e-3
    priv_reg_coef_schedual: [0, 0.1, 2000, 3000]

    max_grad_norm: 1.0
    use_clipped_value_loss: True
    schedule: "adaptive"
    desired_kl: 0.01

    num_steps_per_env: 24
    save_interval: 1000
    logging_interval: 25

    load_optimizer: True

    init_noise_std: 1.0

    num_learning_iterations: 1000000
    init_at_random_ep_len: True
    eval_callbacks: null

    module_dict:      
      actor: 
        input_dim: [actor_obs,"${algo.config.module_dict.actor.motion_encoder.output_dim}","${algo.config.module_dict.actor.history_encoder.output_dim}"]
        output_dim: [robot_action_dim]
        layer_config:
          type: MLP
          hidden_dims: [768, 512, 256]
          activation: SiLU
        use_layernorm: True
        max_sigma: 1.2
        min_sigma: 0.2
        fix_sigma: False

        motion_encoder:
          tsteps: ${obs.future_num_steps}
          input_dim: [future_motion_targets]
          hidden_dim: 60
          output_dim: 128
          layer_config:
            type: Conv1d
            activation: SiLU

        history_encoder:
          tsteps: ${obs.history_length}
          input_dim: [prop_history]
          hidden_dim: 30
          output_dim: 64
          layer_config:
            type: Conv1d
            activation: SiLU

        priv_encoder:
          input_dim: [priv_obs]
          output_dim: ["${algo.config.module_dict.actor.history_encoder.output_dim}"]
          layer_config:
            type: MLP
            hidden_dims: [64]
            activation: SiLU

      critic:
        input_dim: [actor_obs, priv_obs,"${algo.config.module_dict.actor.motion_encoder.output_dim}"]
        output_dim: [1]
        use_layernorm: True
        layer_config:
          type: MLP
          hidden_dims: [768, 512, 256]
          activation: SiLU

      